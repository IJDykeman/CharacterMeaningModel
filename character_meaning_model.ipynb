{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A model for predicting word meaning from characters\n",
    "\n",
    "See my blog post at http://ijdykeman.github.io/ml/2017/01/03/chars-to-vec.html for a description of the model.\n",
    "\n",
    "### This code sets up the data and some functions we'll need later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import random\n",
    "import seaborn as sns\n",
    "import string\n",
    "import csv\n",
    "import string\n",
    "import pickle\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import os\n",
    "from six import moves\n",
    "import ssl\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import collections\n",
    "import functools\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "# you'll need to replace this path with the path to the word vector pickle file\n",
    "with open(\"../training_data/D_cbow_pdw_8B.pkl\", 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "\n",
    "def get_vec(word):\n",
    "    '''\n",
    "    Returns the word vector corresponding to the given word.\n",
    "    '''\n",
    "    if word in model:\n",
    "        return model[word]\n",
    "    else:\n",
    "        return np.zeros_like(model['dog'])\n",
    "    \n",
    "words = model.keys()\n",
    "random.shuffle(words)\n",
    "\n",
    "train_words = words[:int(len(words) * .75)]\n",
    "val_words = words[int(len(words) * .75) : ]\n",
    "\n",
    "seq_max_len = 17 # maximum length of input the model can consider\n",
    "\n",
    "# Create processor for converting strings to one-hot vectors\n",
    "processor = tflearn.data_utils.VocabularyProcessor (seq_max_len, min_frequency=0, vocabulary=None, tokenizer_fn=None)\n",
    "data = [\" \".join(list(word)) for word in model.keys()]\n",
    "num_characters = len(set(\" \".join(data))) + 1\n",
    "processor.fit ([\" \".join(data)+\":;,.'/\"], unused_y=None)\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    '''\n",
    "    returns cosine similarity between vectors a and b\n",
    "    '''\n",
    "    return np.dot(a,b)/(np.sqrt(np.sum(np.power(a,2)))*np.sqrt(np.sum(np.power(b,2))))\n",
    "\n",
    "def sort_topn(l, n):\n",
    "    '''\n",
    "    given a list of tuples (_, value) returns a sorted list of the tuples\n",
    "    with the top n values.\n",
    "    '''\n",
    "    return list(reversed(sorted(l, key = lambda x: x[1])))[:n]\n",
    "\n",
    "all_words = model.keys()\n",
    "all_word_vectors = np.zeros([len(all_words), 500])\n",
    "i=0\n",
    "for word in all_words:\n",
    "    all_word_vectors[i,:] = model[word]\n",
    "    i+=1\n",
    "\n",
    "norms = np.linalg.norm(all_word_vectors, ord=2, axis=1) # the length of each vector in the vocabulary\n",
    "# project all word vectors onto a unit sphere\n",
    "# this way, euclidean nearest neighbor lookup \n",
    "# is the same as finding the vector with the \n",
    "# lowest cosine distance to some vector\n",
    "all_word_vectors = all_word_vectors / norms[:,None] \n",
    "\n",
    "word_tree = KDTree(all_word_vectors)\n",
    "\n",
    "def get_most_similar(vec, exclude = None):\n",
    "    '''\n",
    "    get the words with vectors closes to vec.  Exclude is optionally\n",
    "    a list of words to ignore.\n",
    "    '''\n",
    "    topn = [(\"none\", -2)]\n",
    "    words = []\n",
    "    for i in word_tree.query([vec], k=15)[1][0]:\n",
    "        words.append(all_words[i])\n",
    "    for word in words:\n",
    "        if word != exclude:\n",
    "            topn.append((word, cosine_similarity(vec, model[word])))\n",
    "            # KDtree doesn't return results in sorted order, so we sort them\n",
    "            topn = sort_topn(topn, 15)\n",
    "    return topn\n",
    "\n",
    "def get_characters_vector(word):\n",
    "    '''\n",
    "    Converts string word into a sequence of one-hot vectors.\n",
    "    '''\n",
    "    item = processor.transform( [\" \".join(list(word))]).next()    \n",
    "    result = tflearn.data_utils.to_categorical (item, num_characters)\n",
    "    return result\n",
    "    \n",
    "def get_batch_from_words_list(n, words):\n",
    "    '''\n",
    "    returns a batch of n inputs and outputs drawn from the list words.\n",
    "    '''\n",
    "    x = []\n",
    "    y = []\n",
    "    for _ in range(n):\n",
    "        i = random.randint(0, len(words)-1)\n",
    "        word = words[i]\n",
    "        x.append(get_characters_vector(word))\n",
    "        y.append(get_vec(word))\n",
    "    x = np.array(x)[:,::-1] \n",
    "    return x,y\n",
    "\n",
    "def get_batch(n):\n",
    "    '''\n",
    "    returns a batch of training and validation examples\n",
    "    '''\n",
    "    tx, ty = get_batch_from_words_list(n, train_words)\n",
    "    vx, vy = get_batch_from_words_list(n, val_words)\n",
    "    return tx, ty, vx, vy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "input_chars = tf.placeholder(tf.float32, [None, seq_max_len, num_characters], name = 'inpt_ph')\n",
    "word_vectors = tf.placeholder(tf.float32, [None, len(get_vec('dog'))], name = 'yn_ph')\n",
    "y = word_vectors\n",
    "\n",
    "g = tflearn.lstm(input_chars, 250, return_seq = True)\n",
    "g = tflearn.lstm(g, 250, return_seq = True)\n",
    "g = tflearn.lstm(g, 250, return_seq = False)\n",
    "g = tflearn.fully_connected(g, 400, activation='relu')\n",
    "g = tflearn.layers.dropout(g, .7)\n",
    "pred = tflearn.fully_connected(g, 500, activation='tanh')\n",
    "\n",
    "# This tensor is the length (2 norm) of each vector predicted by the model.  It's needed for the cosine\n",
    "# distance calculation.\n",
    "pred_len = tf.sqrt(tf.reduce_sum(tf.square(pred), 1, keep_dims = True))\n",
    "# The length of the input vectors\n",
    "y_len =    tf.sqrt(tf.reduce_sum(tf.square(y),    1, keep_dims = True))\n",
    "# Cosine distance between each correct vector and predicted vector\n",
    "cosine_similarity_ten = tf.reduce_sum(tf.div(tf.mul(pred, y),   tf.mul(pred_len, y_len)), 1, keep_dims = False)\n",
    "# 1 - cosine similarity = cosine distance.  Cosine distance is our loss.\n",
    "cost = tf.reduce_mean(tf.square(tf.sub(tf.constant(1.0), cosine_similarity_ten)))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=.002).minimize(cost)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "config = tf.ConfigProto(\n",
    "#         device_count = {'GPU': 0} # this disables the GPU for this session\n",
    "    )\n",
    "sess = tf.Session(config=config)\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "step = 0\n",
    "batch_costs = []\n",
    "val_costs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Loss\n",
    "\n",
    "Let's get an idea of what kind of loss a naive model which always guesses the average of vectors in the vocabulary would achieve.  I'll take the average of 2000 vectors then measure the average cosine distance between those vectors and their average.  We'd like to beat this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x,y = get_batch_from_words_list(2000, train_words)  \n",
    "avg = (np.sum(y, axis = 0))/len(y)\n",
    "sims = []\n",
    "for i in range(len(y)):\n",
    "    sims.append((1 - cosine_similarity(y[i], avg))**2)\n",
    "print (\"naive loss:\", np.mean(sims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "display_step = 1000\n",
    "# a bar to show progress toward the next graph printout\n",
    "f = FloatProgress(min=0, max=display_step, readout = True)\n",
    "display(f)\n",
    "\n",
    "\n",
    "while step < 8000:\n",
    "    x,y = get_batch_from_words_list(250, train_words)  \n",
    "    _, batch_cost = sess.run([optimizer, cost], feed_dict={input_chars:x, word_vectors: y })\n",
    "    batch_costs.append(batch_cost)\n",
    "    f.value = step%display_step\n",
    "    \n",
    "    # to save time, only calculate the validation loss every 10 batches.\n",
    "    if step %  10 == 0:\n",
    "        x,y = get_batch_from_words_list(250, val_words)  \n",
    "        val_cost = sess.run([cost], feed_dict={input_chars:x, word_vectors: y })\n",
    "        val_costs.append(val_cost)\n",
    "    if  step % display_step == 0 and step > 0:\n",
    "        plt.plot(batch_costs)\n",
    "        plt.ylabel('train loss')\n",
    "        plt.show()\n",
    "        plt.plot(val_costs)\n",
    "        plt.ylabel('validation loss')\n",
    "        plt.show()\n",
    "\n",
    "    step += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for seeing the model's responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict_vec(letters):\n",
    "    '''\n",
    "    Get the model's prediction of a word vector given the string letters.\n",
    "    '''\n",
    "    letters = letters.lower()\n",
    "    return np.array(sess.run(pred, feed_dict = \n",
    "                             {input_chars:np.array([get_characters_vector(letters)])[:,::-1]})[0])\\\n",
    "           .astype(np.float32)\n",
    "\n",
    "\n",
    "def report(letters):\n",
    "    '''\n",
    "    Print out the model's guess at words similar to the string letters.  Also show the word vectors\n",
    "    closest to the vector for letters if one already exists in the embedding.\n",
    "    '''\n",
    "    print (\"\\nreport for \"+letters)\n",
    "    letters = letters.lower()\n",
    "    if len(letters) > seq_max_len:\n",
    "        print (\"word too long\")\n",
    "        return\n",
    "    guess = predict_vec(letters)\n",
    "    print (\"The most similar words according to the model:\")\n",
    "    print (str(map(lambda x: x[0],get_most_similar(guess))).decode('unicode-escape'))\n",
    "    if letters in model:\n",
    "        print (\"The most similar words according to the original embedding space:\")\n",
    "        print (map(lambda x: x[0],get_most_similar(model[letters])))\n",
    "    else:\n",
    "        print (\"This isn't in the original embedding space.\")\n",
    "    return guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "report(\"fawkes\")\n",
    "report(\"dobby\")\n",
    "report(\"arnold\")\n",
    "report(\"dumbledore\")\n",
    "report(\"Throbfernoma\")\n",
    "report(\"Glazioxx\")\n",
    "report(\"Faeranduil\")\n",
    "report(\"dromgooles\")\n",
    "report(\"beowulf\")\n",
    "report(\"gawain\")\n",
    "report(\"megathron\")\n",
    "report(\"hagrid\")\n",
    "report(\"impairor\")\n",
    "report(\"Cecetzin\")\n",
    "report(\"iteron\") \n",
    "report(\"eddard\")\n",
    "report(\"dursley\")\n",
    "report(\"lothlorien\")\n",
    "report(\"zorgon\")\n",
    "report(\"galadriel\")\n",
    "report(\"goron\")\n",
    "report(\"danolfini\")\n",
    "report(\"ashimmu\")\n",
    "\n",
    "print (\"reports complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
